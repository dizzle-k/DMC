{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook shows how to make use of the goodreads co-occurence rating matrix\n",
    "# Please, download the files 'user_book_matrix.npy' and 'user_information.p' from the drive \n",
    "# and place them in a folder named \"goodreads\".\n",
    "\n",
    "# Disclaimer: goodreads books were matched with dmc books via fuzzy string match of title and author, \n",
    "# some ill-matched cases are possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "described-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, load_npz\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from evaluation_workflow import evaluate_recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-armenia",
   "metadata": {},
   "source": [
    "#### Load DMC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc_task_books = dict()\n",
    "with open(\"DMC-2021-Task/items.csv\") as i:\n",
    "    csvreader = csv.reader(i,delimiter=\"|\")\n",
    "    next(csvreader) # header\n",
    "    for line in csvreader:\n",
    "        itemID, title = line[:2]\n",
    "        dmc_task_books[int(itemID)] = title\n",
    "\n",
    "print(f\"example: {list(dmc_task_books.items())[0]}\")\n",
    "print(f\"all task books: {len(dmc_task_books)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-aircraft",
   "metadata": {},
   "source": [
    "#### Load User Information\n",
    "-> Dictionary of userID and value-list with \"avg rating\" and \"no. of rated book\" per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"goodreads/user_information.p\", \"rb\") as u:\n",
    "    user_information = pickle.load(u)\n",
    "\n",
    "user_information[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-lesson",
   "metadata": {},
   "source": [
    "#### Load DMC Books that are on Goodreads\n",
    "and exclude those, that were clearly matched incorrectly with a one-word-title from goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "from book_classes import DMCBook, GoodreadsBook, GoodreadsAuthor\n",
    "#from book_classes import DMCGenre\n",
    "\n",
    "dmc_to_gr_string = pickle.load(open(\"goodreads/dmc_to_goodreads.p\", \"rb\"))\n",
    "dmc_to_gr = {int(k): int(v) for k,v in dmc_to_gr_string.items()}\n",
    "one_word_titles = {}\n",
    "genres = [\"children_with_matches.p\",\n",
    "          \"comics_graphic_with_matches.p\",\n",
    "          \"dmc_remaining_with_matches.p\",\n",
    "          \"fantasy_paranormal_with_matches.p\",\n",
    "          \"history_biography_with_matches.p\",\n",
    "          \"mystery_thriller_crime_with_matches.p\",\n",
    "          \"romance_with_matches.p\",\n",
    "          \"young_adult_with_matches.p\"]\n",
    "for gf in genres:\n",
    "    with open(f\"goodreads/matched/{gf}\", \"rb\") as g:\n",
    "        try:\n",
    "            dmc_books = pickle.load(g)\n",
    "            for dmcb in dmc_books:\n",
    "                if dmcb.goodreads_match != None:\n",
    "                    gr_title = dmcb.goodreads_match.title\n",
    "                    if len(gr_title) == 1 and len(dmcb.title) != 1:\n",
    "                        one_word_titles[dmcb.itemID] = (dmcb.title, gr_title)\n",
    "        except EOFError:\n",
    "            #print(f\"empty file: {gf}\\n\")\n",
    "            pass\n",
    "\n",
    "print(f\"one word title matches that get excluded: {len(one_word_titles)}\\n\")\n",
    "\n",
    "for item in one_word_titles.keys():\n",
    "        del dmc_to_gr[item]\n",
    "\n",
    "print(f\"all task books on goodreads: {len(dmc_to_gr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-compact",
   "metadata": {},
   "source": [
    "#### Load Book-User-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_book_matrix = np.load(\"goodreads/user_book_matrix.npy\", allow_pickle=True)\n",
    "#user_book_matrix = load_npz(\"goodreads/user_book_matrix.npy\")\n",
    "\n",
    "user_book_matrix = user_book_matrix.item()\n",
    "print(user_book_matrix.shape)\n",
    "print(type(user_book_matrix[0,:]))\n",
    "print(user_book_matrix.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve relevant book columns\n",
    "book_columns = dict()\n",
    "coo_matrix = user_book_matrix.tocoo()\n",
    "ratings_non_zero = set(zip(coo_matrix.row, coo_matrix.col))\n",
    "for book,user in ratings_non_zero:\n",
    "    if int(book) in dmc_to_gr.keys():\n",
    "        book_columns[int(book)] = user_book_matrix[book,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMC books with user ratings\n",
    "len(book_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(book_a, book_b):\n",
    "    return 1 - distance.cosine(book_a, book_b)\n",
    "\n",
    "def compute_top10_recs(itemID, book_columns):\n",
    "    similarity = {}\n",
    "    book_vector = book_columns[itemID].toarray()[0]\n",
    "    for book_id, ratings in book_columns.items():\n",
    "        sim = compute_similarity(book_vector, ratings.toarray()[0].T)\n",
    "        similarity[book_id] = sim\n",
    "    top10 = list(dict(sorted(similarity.items(), key=lambda item: item[1], reverse=True)).items())[1:11]\n",
    "    top10_books = [(b[0], dmc_task_books[b[0]],b[1]) for b in top10]\n",
    "    return top10_books\n",
    "    \n",
    "# todo: add normalization (subtract average user rating for each user in two vectors of interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dragon = 13834\n",
    "fire_and_ice = 54197\n",
    "orange_girl = 39249\n",
    "more_happy_than_not = 8791\n",
    "wizard_oz = 40426\n",
    "\n",
    "print(compute_top10_recs(last_dragon, book_columns))\n",
    "print(\"\\n\")\n",
    "\"\"\"print(compute_top10_recs(fire_and_ice, book_columns))\n",
    "print(\"\\n\")\n",
    "print(compute_top10_recs(orange_girl, book_columns))\n",
    "print(\"\\n\")\n",
    "print(compute_top10_recs(more_happy_than_not, book_columns))\n",
    "print(\"\\n\")\n",
    "print(compute_top10_recs(wizard_oz, book_columns))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-mother",
   "metadata": {},
   "source": [
    "### Load amazon validation and test set and check how much books are covered on goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pressed-score",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "df_val = pd.read_csv('dmc21_amazon_validation.csv')\n",
    "validation_books = list(df_val[\"itemID\"])\n",
    "df_test = pd.read_csv('dmc21_amazon_test.csv')\n",
    "test_books = list(df_test[\"itemID\"])\n",
    "if df_test[\"rec1_ID\"].isnull().values.any():\n",
    "    print(df_test[\"rec1_ID\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions for covered validation books\n",
    "df_val_covered = df_val[df_val['itemID'].isin(book_columns.keys())]\n",
    "\n",
    "books_val_with_preds = dict()\n",
    "c = 0\n",
    "for bookID in df_val_covered[\"itemID\"]:\n",
    "    books_val_with_preds[bookID] = compute_top10_recs(bookID, book_columns)\n",
    "    c += 1\n",
    "    print(f\"done with {c}\", end=\"\\r\")\n",
    "\n",
    "with open(\"amazon_validation_goodreads_predictions\", \"wb\") as f:\n",
    "    pickle.dump(books_val_with_preds,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "marked-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for all/covered validation books\n",
    "\n",
    "with open(\"amazon_validation_goodreads_predictions\", \"rb\") as f:\n",
    "    books_val_with_preds = pickle.load(f)\n",
    "\n",
    "preds_dict_goodreads = dict()\n",
    "preds_dict_all = {k: [] for k in df_val[\"itemID\"].values}\n",
    "for b, recs in books_val_with_preds.items():\n",
    "    recs = [int(r[0]) for r in recs]\n",
    "    preds_dict_goodreads[b] = recs\n",
    "    if b in preds_dict_all.keys():\n",
    "        preds_dict_all[b] = recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extra-bundle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For validation books that are on goodreads:\n",
      "\n",
      "Number of books with recommendations in validation set: 232 of 232\n",
      "Average number of recommendations per book in validation set: 3.9\n",
      "Precision@1: 4.0%\n",
      "Recall@1: 0.62%\n",
      "F1-Measure@1: 1.07%\n",
      "-------\n",
      "Precision@2: 6.0%\n",
      "Recall@2: 2.9%\n",
      "F1-Measure@2: 3.69%\n",
      "-------\n",
      "Precision@3: 4.67%\n",
      "Recall@3: 3.4%\n",
      "F1-Measure@3: 3.73%\n",
      "-------\n",
      "Precision@5: 3.6%\n",
      "Recall@5: 4.69%\n",
      "F1-Measure@5: 3.81%\n",
      "-------\n",
      "Precision@6: 3.67%\n",
      "Recall@6: 5.27%\n",
      "F1-Measure@6: 4.05%\n",
      "-------\n",
      "Precision@7: 3.14%\n",
      "Recall@7: 5.27%\n",
      "F1-Measure@7: 3.7%\n",
      "-------\n",
      "Mean Average Precision: 2.97%\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k):\n",
    "    \"\"\"\n",
    "    Relevancy of items in top-k predicted recommendations.\n",
    "    For cases with no predicted recommendations, precision is automatically 1.\n",
    "    ! Order un-aware metric.\n",
    "    \"\"\"\n",
    "    if len(y_pred) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        y_pred_at_k = y_pred[:k]\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for pred in y_pred_at_k:\n",
    "            if pred in y_true:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        precision_at_k = tp / (tp + fp)\n",
    "        return precision_at_k\n",
    "\n",
    "\n",
    "def recall_at_k(y_true, y_pred, k):\n",
    "    \"\"\"\n",
    "    Coverage of relevant items in top-k predicted recommendations.\n",
    "    For cases with no predicted recommendations, recall is automatically 0.\n",
    "    ! Order un-aware metric.\n",
    "    \"\"\"\n",
    "    if len(y_pred) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        y_pred_at_k = y_pred[:k]\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        for true in y_true:\n",
    "            if true in y_pred_at_k:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        recall_at_k = tp / (tp + fn)\n",
    "        return recall_at_k\n",
    "\n",
    "\n",
    "def f1_score_at_k(precision_at_k, recall_at_k):\n",
    "    \"\"\"\n",
    "    F1 score for k predictions.\n",
    "    ! Order un-aware metric.\n",
    "    \"\"\"\n",
    "    if precision_at_k == 0 and recall_at_k == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (2 * precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
    "\n",
    "\n",
    "def avg_precision(y_true, y_preds):\n",
    "    correct_preds = 0\n",
    "    running_sum = 0\n",
    "    for k in range(len(y_preds)):\n",
    "        if y_preds[k] in y_true:\n",
    "            correct_preds += 1\n",
    "            running_sum += correct_preds / (k + 1)\n",
    "    avg_precision = running_sum / len(y_true)\n",
    "    return avg_precision\n",
    "\n",
    "class ValidationBook:\n",
    "    def __init__(self, b_id, recs):\n",
    "        self.b_id = b_id\n",
    "        self.recs = recs\n",
    "\n",
    "\n",
    "def parse_eval_set(filename):\n",
    "    df_val = pd.read_csv(filename)\n",
    "    val_books = []\n",
    "    for index, row in df_val.iterrows():\n",
    "        b_id = row[0]\n",
    "        recs = []\n",
    "        for rec in row[1:]:\n",
    "            if isinstance(rec, str):\n",
    "                rec_splitted = rec.split()\n",
    "                if len(rec_splitted) > 1:\n",
    "                    recs.extend([int(r) for r in rec_splitted])\n",
    "                else:\n",
    "                    recs.append(int(rec))\n",
    "        if len(recs) > 0:\n",
    "            val_books.append(ValidationBook(b_id, recs))\n",
    "\n",
    "    print(\n",
    "        f\"Number of books with recommendations in validation set: {len(val_books)} of {len(df_val)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average number of recommendations per book in validation set: {round(mean([len(v.recs) for v in val_books]),1)}\"\n",
    "    )\n",
    "\n",
    "    return val_books\n",
    "\n",
    "def compute_metrics(val_books, predictions_dict):\n",
    "    for k in [1, 2, 3, 5, 6, 7]:\n",
    "        precision_at_k_values = []\n",
    "        recall_at_k_values = []\n",
    "        f1_at_k_values = []\n",
    "\n",
    "        if k == 1:\n",
    "            avg_precision_values = []\n",
    "\n",
    "        for vb in val_books:\n",
    "            p = precision_at_k(vb.recs, predictions_dict[vb.b_id], k)\n",
    "            r = recall_at_k(vb.recs, predictions_dict[vb.b_id], k)\n",
    "            f1 = f1_score_at_k(p, r)\n",
    "            precision_at_k_values.append(p)\n",
    "            recall_at_k_values.append(r)\n",
    "            f1_at_k_values.append(f1)\n",
    "\n",
    "            if k == 1:\n",
    "                ap = avg_precision(vb.recs, predictions_dict[vb.b_id])\n",
    "                avg_precision_values.append(ap)\n",
    "\n",
    "        # compute average across validation set\n",
    "        p_at_k = round(mean(precision_at_k_values) * 100, 2)\n",
    "        r_at_k = round(mean(recall_at_k_values) * 100, 2)\n",
    "        f1_at_k = round(mean(f1_at_k_values) * 100, 2)\n",
    "        mean_avg_precision = round(mean(avg_precision_values) * 100, 2)\n",
    "\n",
    "        print(f\"Precision@{k}: {p_at_k}%\")\n",
    "        print(f\"Recall@{k}: {r_at_k}%\")\n",
    "        print(f\"F1-Measure@{k}: {f1_at_k}%\")\n",
    "        print(\"-------\")\n",
    "\n",
    "    print(f\"Mean Average Precision: {mean_avg_precision}%\")\n",
    "\n",
    "print(\"For validation books that are on goodreads:\\n\")\n",
    "val_books = parse_eval_set('dmc21_amazon_validation.csv')\n",
    "val_books_goodreads = [vb for vb in val_books if vb.b_id in preds_dict_goodreads.keys()]\n",
    "compute_metrics(val_books_goodreads,preds_dict_goodreads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "disciplinary-needle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books with recommendations in validation set: 232 of 232\n",
      "Average number of recommendations per book in validation set: 3.9\n",
      "Precision@1: 79.31%\n",
      "Recall@1: 0.13%\n",
      "F1-Measure@1: 0.23%\n",
      "-------\n",
      "Precision@2: 79.74%\n",
      "Recall@2: 0.63%\n",
      "F1-Measure@2: 0.8%\n",
      "-------\n",
      "Precision@3: 79.45%\n",
      "Recall@3: 0.73%\n",
      "F1-Measure@3: 0.8%\n",
      "-------\n",
      "Precision@5: 79.22%\n",
      "Recall@5: 1.01%\n",
      "F1-Measure@5: 0.82%\n",
      "-------\n",
      "Precision@6: 79.24%\n",
      "Recall@6: 1.14%\n",
      "F1-Measure@6: 0.87%\n",
      "-------\n",
      "Precision@7: 79.13%\n",
      "Recall@7: 1.14%\n",
      "F1-Measure@7: 0.8%\n",
      "-------\n",
      "Mean Average Precision: 0.64%\n"
     ]
    }
   ],
   "source": [
    "evaluate_recommender(preds_dict_all,val_set=\"dmc21_amazon_validation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
